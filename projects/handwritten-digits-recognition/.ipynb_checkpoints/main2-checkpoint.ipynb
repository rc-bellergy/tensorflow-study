{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten digits recognition using Tensorflow with Python\n",
    "\n",
    "**References:**\n",
    "\n",
    "(Chinese) http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist_beginners.html\n",
    "\n",
    "(English) https://dataplatform.cloud.ibm.com/analytics/notebooks/91440c8b-0bfb-471e-b04e-235e4d9f510d/view?access_token=fb4380415a903111e26cec3bd95d8ba91a04746185c866fecde9d36643fa5585\n",
    "\n",
    "1 SEP 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the MNIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tensorflow to environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method **tf.placeholder** allows us to create variables that act as nodes holding the data. Here, x is a 2-dimensionall array holding the MNIST images, with none implying the batch size (which can be of any size) and 784 being a single 28×28 image. y_ is the target output class that consists of a 2-dimensional array of 10 classes (denoting the numbers 0-9) that identify what digit is stored in each image.\n",
    "\n",
    "x不是一個特定的值，而是一個佔位符placeholder，我們在TensorFlow運行計算時輸入這個值。我們希望能夠輸入任意數量的MNIST圖像，每一張圖展平成784維的向量。我們用2維的浮點數張量來表示這些圖，這個張量的形狀是[None，784 ]。（這裡的None表示此張量的第一個維度可以是任何長度的。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, W is the **weight** and b is the **bias** of the model. They are initialized with **tf.Variable** as they are components of the computational graph that need to change values with the input of each different neuron.\n",
    "\n",
    "我們賦予tf.Variable不同的初值來創建不同的Variable：在這裡，我們都用全為零的張量來初始化W和b。因為我們要學習W和b的值，它們的初值可以隨意設置。\n",
    "\n",
    "注意，W的維度是[784，10]，因為我們想要用784維的圖片向量乘以它以得到一個10維的證據值向量，每一位對應不同數字類。b的形狀是[10]，所以我們可以直接把它加到輸出上面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a simple **softmax** model to implement our network. Softmax is a generalization of logistic regression, usually used in the final layer of a network. It is useful because it helps in **multi-classification** models where a given output can be a list of many different things.\n",
    "\n",
    "It provides values between 0 to 1 that in addition give you the probability of the output belonging to a particular class.\n",
    "\n",
    "![](http://wiki.jikexueyuan.com/project/tensorflow-zh/images/mnist1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了訓練我們的模型，我們首先需要定義一個指標來評估這個模型是好的。其實，在機器學習，我們通常定義指標來表示一個模型是壞的，這個指標稱為成本（cost）或損失（loss），然後儘量最小化這個指標。但是，這兩種方式是相同的。\n",
    "\n",
    "一個非常常見的，非常漂亮的成本函數是“交叉熵”（cross-entropy）。交叉熵產生於信息論裡面的信息壓縮編碼技術，但是它後來演變成為從博弈論到機器學習等其他領域裡的重要技術手段。它的定義如下：\n",
    "![](http://wiki.jikexueyuan.com/project/tensorflow-zh/images/mnist10.png)\n",
    "y是我們預測的概率分佈, y'是實際的分佈（我們輸入的one-hot vector)。比較粗糙的理解是，交叉熵是用來衡量我們的預測用於描述真相的低效性。更詳細的關於交叉熵的解釋超出本教程的範疇，但是你很有必要好好[理解它](http://colah.github.io/posts/2015-09-Visual-Information/)。\n",
    "\n",
    "為了計算交叉熵，我們首先需要添加一個新的placeholder用於輸入正確值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(\"float\", [None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算交叉熵 (cross-entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，用tf.log計算y的每個元素的對數。接下來，我們把y_的每一個元素和tf.log(y)的對應元素相乘。最後，用tf.reduce_sum計算張量的所有元素的總和。（注意，這裡的交叉熵不僅僅用來衡量單一的一對預測和真實值，而是所有100幅圖片的交叉熵的總和。對於100個數據點的預測表現比單一數據點的表現能更好地描述我們的模型的性能。\n",
    "\n",
    "現在我們知道我們需要我們的模型做什麼啦，用TensorFlow來訓練它是非常容易的。因為TensorFlow擁有一張描述你各個計算單元的圖，它可以自動地使用反向傳播算法(backpropagation algorithm)來有效地確定你的變量是如何影響你想要最小化的那個成本值的。然後，TensorFlow會用你選擇的優化算法來不斷地修改變量以降低成本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這裡，我們要求TensorFlow用梯度下降算法（gradient descent algorithm）以0.01的學習速率最小化交叉熵(cross-entropy)。梯度下降算法（gradient descent algorithm）是一個簡單的學習過程，TensorFlow只需將每個變量一點點地往使成本不斷降低的方向移動。當然TensorFlow也提供了其他許多優化算法：只要簡單地調整一行代碼就可以使用其他的算法。\n",
    "\n",
    "TensorFlow在這裡實際上所做的是，它會在後台給描述你的計算的那張圖裡面增加一系列新的計算操作單元用於實現反向傳播算法和梯度下降算法。然後，它返回給你的只是一個單一的操作，當運行這個操作時，它用梯度下降算法訓練你的模型，微調你的變量，不斷減少成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們需要添加一個操作來初始化我們創建的變量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the session\n",
    "現在我們可以在一個Session里面啟動我們的模型，並且初始化變量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't know what the **tf.Session** is.\n",
    "https://www.tensorflow.org/guide/graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然後開始訓練模型，這裡我們讓模型循環訓練1000次！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "該循環的每個步驟中，我們都會隨機抓取訓練數據中的100個批處理數據點，然後我們用這些數據點作為參數替換之前的佔位符來運行train_step。\n",
    "\n",
    "使用一小部分的隨機數據來進行訓練被稱為隨機訓練（stochastic training）- 在這裡更確切的說是隨機梯度下降訓練。在理想情況下，我們希望用我們所有的數據來進行每一步的訓練，因為這能給我們更好的訓練結果，但顯然這需要很大的計算開銷。所以，每一次訓練我們可以使用不同的數據子集，這樣做既可以減少計算開銷，又可以最大化地學習到數據集的總體特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Model\n",
    "那麼我們的模型性能如何呢？\n",
    "\n",
    "首先讓我們找出那些預測正確的標籤。tf.argmax是一個非常有用的函數，它能給出某個tensor對像在某一維上的其數據最大值所在的索引值。由於標籤向量是由0,1組成，因此最大值1所在的索引位置就是類別標籤，比如tf.argmax(y,1)返回的是模型對於任一輸入x預測到的標籤值，而tf.argmax(y_,1)代表正確的標籤，我們可以用tf.equal來檢測我們的預測是否真實標籤匹配(索引位置一樣表示匹配)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這行代碼會給我們一組布爾值。為了確定正確預測項的比例，我們可以把布爾值轉換成浮點數，然後取平均值。例如，[True, False, True, True]會變成[1,0,1,1]，取平均值後得到0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後，我們計算所學習到的模型在測試數據集上面的正確率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8719\n"
     ]
    }
   ],
   "source": [
    "print (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個最終結果值應該大約是91%。\n",
    "\n",
    "這個結果好嗎？嗯，並不太好。事實上，這個結果是很差的。這是因為我們僅僅使用了一個非常簡單的模型。不過，做一些小小的改進，我們就可以得到97％的正確率。最好的模型甚至可以獲得超過99.7％的準確率！（想了解更多信息，可以看看這個關於各種模型的性能對比列表。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
